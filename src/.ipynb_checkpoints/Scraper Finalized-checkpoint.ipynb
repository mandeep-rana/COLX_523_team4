{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "#regex for extracting decimals from https://stackoverflow.com/questions/12117024/decimal-number-regular-expression-where-digit-after-decimal-is-optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name='d2.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_table=\"\"\"CREATE TABLE if not exists \"courses\" (\n",
    "\t\"url\"\tTEXT UNIQUE,\n",
    "\t\"title\"\tTEXT,\n",
    "\t\"instructor\"\tTEXT,\n",
    "\t\"average_rating\"\tREAL,\n",
    "\t\"total_pages\"\tINTEGER,\n",
    "\t\"scraped_pages\"\tINTEGER,\n",
    "\tPRIMARY KEY(\"url\")\n",
    ");\"\"\"\n",
    "\n",
    "reviews_table=\"\"\"\n",
    "CREATE TABLE if not exists \"reviews\" (\n",
    "\t\"id\"\tTEXT UNIQUE,\n",
    "\t\"course_url\"\tINTEGER,\n",
    "\t\"reviewer\"\tTEXT,\n",
    "\t\"review_date\"\tTEXT,\n",
    "\t\"review_unix\"\tREAL,\n",
    "\t\"helpful_count\"\tINTEGER,\n",
    "\t\"rating\"\tREAL,\n",
    "\t\"review\"\tINTEGER,\n",
    "\tPRIMARY KEY(\"id\")\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_db():\n",
    "    #this function creates a new database and creates the two empty tables\n",
    "    conn=sqlite3.connect(db_name)\n",
    "    cur=conn.cursor()\n",
    "    cur.execute(courses_table)\n",
    "    cur.execute(reviews_table)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def scrape_course_main_page(course_url,cur,conn,debug=False):\n",
    "    #this function checks the main page of a course and fetch the meta data\n",
    "    html=requests.get(course_url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    instructor=soup.find('div',attrs={'class':'rc-BannerInstructorInfo'}).find('span').text.strip()\n",
    "    average_rating=soup.find('span',attrs={'class':'number-rating'}).text.strip()\n",
    "    average_rating=re.findall(r'\\d+\\.?\\d*',average_rating)[0]\n",
    "    title=soup.find('div',attrs={'class':'BannerTitle'}).find('h1').text.strip()\n",
    "    \n",
    "    review_html=requests.get(course_url+'/reviews').content\n",
    "    review_soup=BeautifulSoup(review_html)\n",
    "    #this code below finds the navigation bar, and the last element is the total number of pages of reviews\n",
    "    pg_nav_bar=review_soup.find('nav',attrs={'aria-label':'Pagination Controls'}).findAll('span')\n",
    "    total_pages=int(pg_nav_bar[-2].text)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Course title: \",title)\n",
    "        print(\"Instructor: \",instructor)\n",
    "        print(\"Average course rating: \",average_rating)\n",
    "        print(\"Total pages: \",total_pages)\n",
    "    \n",
    "    cur.execute(\"update courses set title=?, instructor=?, average_rating=?, total_pages=? where url=?\", (title,instructor,average_rating,total_pages,course_url))\n",
    "    conn.commit()\n",
    "    \n",
    "def scrape_course_review(course_url,pgn,cur,conn,debug=False):\n",
    "    #this funtion will scrape exactly one page of course review, denoted by the page number\n",
    "    if pgn==400:\n",
    "        return#page 400 and after are not accessible\n",
    "    review_url=course_url+'/reviews?page={}'.format(pgn)\n",
    "    html=requests.get(review_url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    for review in soup.findAll('div',attrs={'class':'review-page-review'}):\n",
    "        #for each review ...\n",
    "        helpful_count=review.find('button',attrs={'class':'review-helpful-button'}).text\n",
    "        helpful_count=re.findall(r'\\d+',helpful_count)\n",
    "        #sometimes no one will upvote a review as \"helpful\", thus regex won't find any number\n",
    "        if helpful_count!=[]:\n",
    "            helpful_count=helpful_count[0]\n",
    "        else:\n",
    "            helpful_count=0\n",
    "        review_text=review.find('div',attrs={'class':'reviewText'}).text\n",
    "        reviewer=review.find('p',attrs={'class':'reviewerName'}).text.replace(\"By\",\"\").strip()\n",
    "        review_date=review.find('div',attrs={'class':'dateOfReview'}).text\n",
    "        review_timestamp=datetime.datetime.strptime(review_date,'%b %d, %Y').timestamp()\n",
    "        star_box=review.find('div',attrs={'role':'img'})\n",
    "        star_count=len(star_box.findAll('title',string='Filled Star'))\n",
    "        if debug:\n",
    "            print(\"Reviwer: \",reviewer)\n",
    "            print(\"Review Date: \",review_date)\n",
    "            print(\"Upvote: \",helpful_count)\n",
    "            print(\"Rating: \",star_count)\n",
    "            print(\"Review: \",review_text)\n",
    "        #use md5 hashing to create an unique id for each review\n",
    "        uid=review_url+reviewer+review_date+review_text\n",
    "        uid=hashlib.md5(uid.encode('utf-8')).hexdigest()\n",
    "        cur.execute(\"INSERT INTO reviews (id,course_url,reviewer,review_date,review_unix,helpful_count,rating,review) values (?,?,?,?,?,?,?,?) on CONFLICT(id) DO UPDATE SET  id=id\",(uid,course_url,reviewer,review_date,review_timestamp,helpful_count,star_count,review_text))\n",
    "        conn.commit()\n",
    "    cur.execute(\"update courses set scraped_pages=? where url=?\",(pgn,course_url))\n",
    "    conn.commit()\n",
    "    \n",
    "def scrape_course_list(pgn,cur,conn,debug=False):\n",
    "    #this function scrapes exactly one page of courses, denoted by pgn\n",
    "    list_url='https://www.coursera.org/search?query=free&page={}&index=prod_all_products_term_optimization&allLanguages=English'.format(pgn)\n",
    "    html=requests.get(list_url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    #the course list is actually returned by the webserver as a json object\n",
    "    list_obj=json.loads(soup.find('script',attrs={'type':'application/ld+json'}).string)\n",
    "    for course in list_obj['itemListElement']:\n",
    "        cur.execute(\"INSERT INTO courses (url) Values (?) on CONFLICT(url) DO UPDATE SET  url=url\",(course['url'],))\n",
    "        conn.commit()\n",
    "        if debug:\n",
    "            print(course['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_course_meta_data(to_scrape_course_page=True,to_scrape_course_list=True,pages_of_courses=10):\n",
    "    #this function calls some of the above functions to scrape the meta data\n",
    "    #calls initialize db to initialize data base\n",
    "    initialize_db()\n",
    "    conn=sqlite3.connect(db_name)\n",
    "    cur=conn.cursor()\n",
    "    #if we choose to scrape the course list from coursera\n",
    "    if to_scrape_course_list:\n",
    "        #then we plan to scrape 10 pages of courses\n",
    "        for i in range(1,pages_of_courses+1):\n",
    "            #for each page number, call scrape course list function and provide the page number to scrape\n",
    "            #the function will store the course url in the database\n",
    "            scrape_course_list(i,cur,conn)\n",
    "            time.sleep(5)\n",
    "    \n",
    "    #if we choose to scrape the meta data for courses\n",
    "    if to_scrape_course_page:\n",
    "        #then for all of the course urls that we collected above\n",
    "        for i in cur.execute(\"select url from courses where title is null\").fetchall():\n",
    "            course_url=i[0]\n",
    "            #call the function to scrape meta data for that course, such as the instructor name etc..\n",
    "            scrape_course_main_page(course_url,cur,conn)\n",
    "            time.sleep(5)\n",
    "        \n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews():\n",
    "    conn=sqlite3.connect(db_name)\n",
    "    cur=conn.cursor()\n",
    "    #after we scraped the meta data for each course, it's time to scrape the reviews\n",
    "    #for each course that we collected meta data above\n",
    "    for i in cur.execute(\"select url,total_pages,scraped_pages from courses where total_pages is not null\").fetchall():\n",
    "        course_url,total_pages,scraped_pages=i\n",
    "        #scraped pages is initialized to None\n",
    "        #after each page of review is scraped, this column is updated\n",
    "        #this scraped pages is a \"progress\" counter\n",
    "        #if the number of scraped pages equals to total pages of reviews, then we are done for this particular course\n",
    "        if total_pages==scraped_pages:\n",
    "            continue\n",
    "        if scraped_pages==None:\n",
    "            scraped_pages=0\n",
    "        #iterate through the page numbers for pages that are yet to be scraped\n",
    "        for pgn in range(scraped_pages+1,total_pages+1):\n",
    "            if pgn>10:#only scrape 10 pages for each course to have some diversity\n",
    "                continue\n",
    "            #call scrape review function to scrape reviews in that page\n",
    "            scrape_course_review(course_url,pgn,cur,conn)\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_names_and_export():\n",
    "    conn=sqlite3.connect(db_name)\n",
    "    cur=conn.cursor()\n",
    "    export_data=[]\n",
    "    #for each piece of review collected, only extract id, review text, and instructor name\n",
    "    for i in cur.execute(\"select id,review,instructor from reviews left join courses on courses.url = reviews.course_url\").fetchall():\n",
    "        uid,review,instructor=i\n",
    "        #some instructor name list is too long, thus coursera adds '+n  more instructors' at the end\n",
    "        #those irrelevant strings must be removed\n",
    "        instructor=instructor.replace('more instructor','').replace('more instructors','')\n",
    "        #the if statement is removing '+n' in the instructor name string\n",
    "        instructor_names=[i for i in instructor.split() if i[0]!='+']\n",
    "        #some reviews only have number and need to be thrown out\n",
    "        if type(review) == type(5):\n",
    "            continue\n",
    "        #strip the excessive spaces\n",
    "        review=review.strip()\n",
    "        for name_candidate in instructor_names:\n",
    "            review=review.replace(name_candidate,'')\n",
    "        export_data.append({'id':uid,'review':review})\n",
    "    export_data=pd.DataFrame(export_data)\n",
    "    export_data.to_csv('exported_data.tsv',sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_course_meta_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_names_and_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
