# Annotation Readme

## Links

**Code**
1. [Sampling code](): extract samples from the corpus for annotation

**Sampled corpus**</br>
We sampled 3 small sets of reviews (~15 reviews each) to try different settings on Mechanical Turk that might result in better annotation:
1. [Control group]()
    - Used the original annotation guidelines completed in milestone 2
    - No restrictions on annotator approval rate
    - \$0.03 per task (moderate pay)
2. [High pay high standard]()
    - Used the original annotation guidelines completed in milestone 2, with emphasize on multi-label
    - Annotator approval rate must be >80%
    - 0.05 per task (high pay)
3. [Simple rules]()
    - Simplified annotation guidelines to improve readability, with emphasize on multi-label
    - No restrictions on annotator approval rate
    - \$0.03 per task (moderate pay)</br>
For the final annotation we chose the second approach: high pay and high standard (for details please see the [interannotator agreement study]())

Link to any intermediary files produced during the annotation process (e.g. csvs downloaded from mechanical turk, excel documents for individual annotators, etc.)


sample data - for method testing
0: high pay high standard - \$0.05 requiring HIT approval rate > 80
1: simplistic rule - reduce total number of examples but add examples showing multiple tagging